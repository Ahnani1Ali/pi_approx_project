{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation du Nombre $\\pi$ par des MÃ©thodes de Machine Learning\n",
    "\n",
    "**M1 Informatique â€” SpÃ©cialitÃ© Intelligence Artificielle**  \n",
    "**MathÃ©matiques AppliquÃ©es au Machine Learning**\n",
    "\n",
    "---\n",
    "\n",
    "Ce notebook implÃ©mente et compare quatre approches d'approximation de $\\pi$ :\n",
    "\n",
    "1. **Monte-Carlo stochastique** â€” estimateur empirique basÃ© sur la loi des grands nombres\n",
    "2. **RÃ©gression polynomiale** â€” approximation fonctionnelle par moindres carrÃ©s\n",
    "3. **RÃ©seau de neurones MLP** â€” approximation universelle (Cybenko 1989, Hornik 1991)\n",
    "4. **Processus Gaussien** â€” infÃ©rence bayÃ©sienne non-paramÃ©trique (Rasmussen & Williams 2006)\n",
    "\n",
    "**Dataset** : donnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©es depuis la relation gÃ©omÃ©trique $y = \\sqrt{1-x^2}$,  \n",
    "de sorte que $\\pi = 4\\int_0^1 \\sqrt{1-x^2}\\,dx$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import time\n",
    "\n",
    "# â”€â”€â”€ Configuration matplotlib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rcParams['figure.dpi'] = 130\n",
    "rcParams['font.family'] = 'DejaVu Sans'\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "\n",
    "COLORS = {\n",
    "    'mc':   '#1f4e79',   # Navy Blue\n",
    "    'poly': '#1d6b2e',   # Forest Green\n",
    "    'mlp':  '#8b0000',   # Dark Red\n",
    "    'gp':   '#d4700a',   # Orange\n",
    "    'true': '#555555',   # Gray\n",
    "}\n",
    "\n",
    "PI = np.pi\n",
    "print(f\"Valeur de rÃ©fÃ©rence : Ï€ = {PI:.15f}\")\n",
    "print(f\"Packages chargÃ©s avec succÃ¨s âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. GÃ©nÃ©ration du Dataset SynthÃ©tique\n",
    "\n",
    "**Principe gÃ©omÃ©trique :** L'aire d'un demi-disque unitaire est $\\pi/2$, soit :\n",
    "$$\\int_0^1 \\sqrt{1-x^2}\\,dx = \\frac{\\pi}{4}$$\n",
    "\n",
    "Nous observons la fonction $f(x) = \\sqrt{1-x^2}$ en $n$ points avec bruit gaussien :\n",
    "$$y_i = \\sqrt{1-x_i^2} + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n: int, sigma_noise: float = 0.01, seed: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re le dataset synthÃ©tique basÃ© sur le quart de cercle.\n",
    "    \n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    n            : nombre de points\n",
    "    sigma_noise  : Ã©cart-type du bruit gaussien\n",
    "    seed         : graine alÃ©atoire pour reproductibilitÃ©\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    X : array (n,) â€” abscisses dans [0, 1]\n",
    "    y : array (n,) â€” ordonnÃ©es bruitÃ©es\n",
    "    y_true : array (n,) â€” ordonnÃ©es exactes\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.sort(rng.uniform(0, 1, n))\n",
    "    y_true = np.sqrt(1 - X**2)\n",
    "    y = y_true + rng.normal(0, sigma_noise, n)\n",
    "    return X, y, y_true\n",
    "\n",
    "# Dataset principal\n",
    "N_TRAIN = 300\n",
    "SIGMA    = 0.01\n",
    "X_train, y_train, y_clean = generate_dataset(N_TRAIN, SIGMA)\n",
    "\n",
    "# Grille de test dense\n",
    "X_test  = np.linspace(0, 1, 1000)\n",
    "y_exact = np.sqrt(1 - X_test**2)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# (a) DonnÃ©es + courbe exacte\n",
    "ax = axes[0]\n",
    "ax.scatter(X_train, y_train, s=8, alpha=0.5, color=COLORS['mc'], label=f'DonnÃ©es bruitÃ©es ($n={N_TRAIN}$, $\\sigma={SIGMA}$)')\n",
    "ax.plot(X_test, y_exact, color=COLORS['true'], lw=2, label=r'$f(x)=\\sqrt{1-x^2}$ (exacte)')\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.set_ylabel('$y$', fontsize=12)\n",
    "ax.set_title('Dataset synthÃ©tique : quart de cercle', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# (b) InterprÃ©tation gÃ©omÃ©trique\n",
    "ax = axes[1]\n",
    "theta = np.linspace(0, np.pi/2, 500)\n",
    "ax.fill_between(np.cos(theta), np.sin(theta), alpha=0.15, color=COLORS['gp'], label=f'Aire = $\\pi/4 \\\\approx$ {PI/4:.4f}')\n",
    "ax.plot(np.cos(theta), np.sin(theta), color=COLORS['mc'], lw=2.5, label='Quart de cercle')\n",
    "ax.plot([0,1,1,0,0],[0,0,1,1,0], 'k--', lw=1, alpha=0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-0.05, 1.1); ax.set_ylim(-0.05, 1.1)\n",
    "ax.set_xlabel('$x$', fontsize=12); ax.set_ylabel('$y$', fontsize=12)\n",
    "ax.set_title(r'$\\int_0^1\\sqrt{1-x^2}\\,dx = \\pi/4$', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_dataset.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Dataset : {N_TRAIN} points, SNR â‰ˆ {(1/(2*SIGMA)):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. MÃ©thode 1 : Monte-Carlo Stochastique\n",
    "\n",
    "**Estimateur :**\n",
    "$$\\hat{\\pi}_n^{\\text{MC}} = \\frac{4}{n}\\sum_{i=1}^n \\mathbf{1}_{U_{1,i}^2 + U_{2,i}^2 \\leq 1}, \\quad (U_1,U_2)\\sim\\mathcal{U}([0,1]^2)$$\n",
    "\n",
    "**Convergence (TCL) :** $\\sqrt{n}(\\hat{\\pi}_n - \\pi) \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,\\, \\pi(4-\\pi))$\n",
    "\n",
    "**Taux :** $\\mathbb{E}[(\\hat{\\pi}_n - \\pi)^2] = O(n^{-1})$, erreur en $O(n^{-1/2})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_pi(n: int, seed: int = 0) -> tuple:\n",
    "    \"\"\"\n",
    "    Estime Ï€ par la mÃ©thode de Monte-Carlo.\n",
    "    Retourne : (estimation, erreur_std, points_dans_cercle, tous_les_points)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pts = rng.uniform(0, 1, (n, 2))\n",
    "    inside = (pts[:, 0]**2 + pts[:, 1]**2) <= 1.0\n",
    "    pi_hat = 4 * inside.mean()\n",
    "    # IC 95% asymptotique\n",
    "    p_hat = inside.mean()\n",
    "    std_pi = 4 * np.sqrt(p_hat * (1 - p_hat) / n)\n",
    "    return pi_hat, std_pi, inside, pts\n",
    "\n",
    "# â”€â”€ Convergence sur diffÃ©rentes tailles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ns = np.logspace(1, 6, 60, dtype=int)\n",
    "pi_mc_list, std_mc_list = [], []\n",
    "\n",
    "for n in ns:\n",
    "    pi_hat, std, _, _ = monte_carlo_pi(n, seed=42)\n",
    "    pi_mc_list.append(pi_hat)\n",
    "    std_mc_list.append(std)\n",
    "\n",
    "pi_mc_arr = np.array(pi_mc_list)\n",
    "std_mc_arr = np.array(std_mc_list)\n",
    "errors_mc  = np.abs(pi_mc_arr - PI)\n",
    "\n",
    "# â”€â”€ Visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# (a) Points Monte-Carlo (n=5000)\n",
    "ax = axes[0]\n",
    "pi_demo, _, inside_demo, pts_demo = monte_carlo_pi(5000, seed=1)\n",
    "ax.scatter(pts_demo[inside_demo, 0], pts_demo[inside_demo, 1],\n",
    "           s=1.5, alpha=0.4, color=COLORS['mc'], label='Dans le cercle')\n",
    "ax.scatter(pts_demo[~inside_demo, 0], pts_demo[~inside_demo, 1],\n",
    "           s=1.5, alpha=0.4, color='#e05c5c', label='Hors du cercle')\n",
    "theta = np.linspace(0, np.pi/2, 300)\n",
    "ax.plot(np.cos(theta), np.sin(theta), 'k-', lw=2)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(f'Monte-Carlo (n=5000)\\n$\\hat{{\\\\pi}}$ = {pi_demo:.5f}', fontweight='bold')\n",
    "ax.legend(fontsize=8, markerscale=4)\n",
    "\n",
    "# (b) Convergence log-log\n",
    "ax = axes[1]\n",
    "ax.loglog(ns, errors_mc, color=COLORS['mc'], lw=1.5, label='Erreur MC')\n",
    "ax.fill_between(ns, np.maximum(errors_mc - std_mc_arr, 1e-8),\n",
    "                errors_mc + std_mc_arr, alpha=0.2, color=COLORS['mc'])\n",
    "# Droite thÃ©orique O(n^-0.5)\n",
    "ref_x = np.array([ns[0], ns[-1]])\n",
    "ax.loglog(ref_x, 2.0 * ref_x**(-0.5), 'k--', lw=1.5, label=r'$O(n^{-1/2})$')\n",
    "ax.set_xlabel('$n$ (nombre de points)', fontsize=11)\n",
    "ax.set_ylabel('$|\\hat{\\\\pi}_n - \\pi|$', fontsize=11)\n",
    "ax.set_title('Convergence Monte-Carlo', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (c) Distribution de l'estimateur (TCL)\n",
    "ax = axes[2]\n",
    "N_SIMU = 2000\n",
    "n_fixed = 1000\n",
    "pi_samples = [monte_carlo_pi(n_fixed, seed=s)[0] for s in range(N_SIMU)]\n",
    "pi_samples = np.array(pi_samples)\n",
    "ax.hist(pi_samples, bins=50, density=True, color=COLORS['mc'],\n",
    "        alpha=0.7, edgecolor='white', label=f'$n={n_fixed}$, {N_SIMU} tirages')\n",
    "# DensitÃ© thÃ©orique (TCL)\n",
    "sigma_th = np.sqrt(PI * (4 - PI) / n_fixed)\n",
    "x_gauss  = np.linspace(pi_samples.min(), pi_samples.max(), 300)\n",
    "from scipy.stats import norm\n",
    "ax.plot(x_gauss, norm.pdf(x_gauss, PI, sigma_th), 'r-', lw=2,\n",
    "        label=r'$\\mathcal{N}(\\pi,\\, \\pi(4-\\pi)/n)$ (TCL)')\n",
    "ax.axvline(PI, color='k', lw=1.5, linestyle='--', label=f'$\\pi$ = {PI:.5f}')\n",
    "ax.set_xlabel('$\\hat{\\\\pi}_n$', fontsize=11)\n",
    "ax.set_ylabel('DensitÃ©', fontsize=11)\n",
    "ax.set_title('Distribution de $\\hat{\\\\pi}_n$ (validation TCL)', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_montecarlo.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "pi_mc_final, std_final, _, _ = monte_carlo_pi(100_000)\n",
    "print(f\"Monte-Carlo (n=100,000) : Ï€Ì‚ = {pi_mc_final:.8f}\")\n",
    "print(f\"  Erreur absolue : {abs(pi_mc_final - PI):.2e}\")\n",
    "print(f\"  IC 95% : [{pi_mc_final - 1.96*std_final:.6f}, {pi_mc_final + 1.96*std_final:.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. MÃ©thode 2 : RÃ©gression Polynomiale\n",
    "\n",
    "**Principe :** Approximer $f(x) = \\sqrt{1-x^2}$ par un polynÃ´me $p_d(x) = \\sum_{k=0}^d \\theta_k x^k$, puis estimer $\\pi$ par intÃ©gration :\n",
    "$$\\hat{\\pi}_{\\text{poly}} = 4\\int_0^1 p_d(x)\\,dx = 4\\sum_{k=0}^d \\frac{\\theta_k}{k+1}$$\n",
    "\n",
    "**Estimateur OLS :** $\\hat{\\bm{\\theta}} = (\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top y$ (rÃ©gression Ridge)\n",
    "\n",
    "**SÃ©lection du degrÃ© :** Validation croisÃ©e $K$-fold minimisant l'erreur de gÃ©nÃ©ralisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialPiEstimator:\n",
    "    \"\"\"\n",
    "    Estimateur de Ï€ par rÃ©gression polynomiale Ridge.\n",
    "    \n",
    "    Attributs\n",
    "    ---------\n",
    "    degree  : degrÃ© polynomial optimal\n",
    "    alpha   : paramÃ¨tre de rÃ©gularisation Ridge\n",
    "    pipeline : sklearn Pipeline (PolynomialFeatures + Ridge)\n",
    "    pi_hat  : estimation de Ï€\n",
    "    theta   : coefficients ajustÃ©s\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_degree: int = 15, alpha: float = 1e-10):\n",
    "        self.max_degree = max_degree\n",
    "        self.alpha = alpha\n",
    "        self.best_degree = None\n",
    "        self.pipeline = None\n",
    "        self.cv_scores = {}\n",
    "    \n",
    "    def select_degree_cv(self, X: np.ndarray, y: np.ndarray, cv: int = 5) -> int:\n",
    "        \"\"\"SÃ©lection du degrÃ© optimal par validation croisÃ©e K-fold.\"\"\"\n",
    "        degrees = range(2, self.max_degree + 1)\n",
    "        for d in degrees:\n",
    "            pipe = Pipeline([\n",
    "                ('poly', PolynomialFeatures(degree=d, include_bias=True)),\n",
    "                ('ridge', Ridge(alpha=self.alpha))\n",
    "            ])\n",
    "            scores = cross_val_score(pipe, X.reshape(-1,1), y,\n",
    "                                     cv=cv, scoring='neg_mean_squared_error')\n",
    "            self.cv_scores[d] = -scores.mean()\n",
    "        self.best_degree = min(self.cv_scores, key=self.cv_scores.get)\n",
    "        return self.best_degree\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'PolynomialPiEstimator':\n",
    "        \"\"\"EntraÃ®ne le modÃ¨le et estime Ï€ par intÃ©gration analytique.\"\"\"\n",
    "        if self.best_degree is None:\n",
    "            self.select_degree_cv(X, y)\n",
    "        \n",
    "        self.pipeline = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=self.best_degree, include_bias=True)),\n",
    "            ('ridge', Ridge(alpha=self.alpha))\n",
    "        ])\n",
    "        self.pipeline.fit(X.reshape(-1,1), y)\n",
    "        \n",
    "        # Coefficients Î¸_k : p_d(x) = Î£ Î¸_k x^k\n",
    "        self.theta = self.pipeline.named_steps['ridge'].coef_.copy()\n",
    "        self.theta[0] = self.pipeline.named_steps['ridge'].intercept_\n",
    "        \n",
    "        # IntÃ©grale analytique : âˆ«â‚€Â¹ Î£ Î¸_k x^k dx = Î£ Î¸_k / (k+1)\n",
    "        k_vals = np.arange(len(self.theta))\n",
    "        self.pi_hat = 4.0 * np.sum(self.theta / (k_vals + 1))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.pipeline.predict(X.reshape(-1,1))\n",
    "\n",
    "\n",
    "# â”€â”€ EntraÃ®nement et sÃ©lection de degrÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t0 = time.time()\n",
    "poly_est = PolynomialPiEstimator(max_degree=15, alpha=1e-12)\n",
    "poly_est.fit(X_train, y_train)\n",
    "t_poly = time.time() - t0\n",
    "\n",
    "print(f\"DegrÃ© optimal sÃ©lectionnÃ© (CV-5) : d = {poly_est.best_degree}\")\n",
    "print(f\"Ï€Ì‚_poly = {poly_est.pi_hat:.10f}\")\n",
    "print(f\"Erreur absolue : {abs(poly_est.pi_hat - PI):.2e}\")\n",
    "print(f\"Temps : {t_poly:.3f}s\")\n",
    "\n",
    "# â”€â”€ Visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# (a) Ajustement polynomial\n",
    "ax = axes[0]\n",
    "y_pred_poly = poly_est.predict(X_test)\n",
    "ax.scatter(X_train, y_train, s=8, alpha=0.4, color=COLORS['poly'], zorder=2, label='DonnÃ©es')\n",
    "ax.plot(X_test, y_exact, color=COLORS['true'], lw=2, label='$f(x)$ exacte')\n",
    "ax.plot(X_test, y_pred_poly, color=COLORS['poly'], lw=2, linestyle='--',\n",
    "        label=f'Poly. d={poly_est.best_degree}')\n",
    "ax.set_xlabel('$x$', fontsize=11); ax.set_ylabel('$y$', fontsize=11)\n",
    "ax.set_title(f'RÃ©gression polynomiale (d={poly_est.best_degree})', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# (b) Courbe MSE vs degrÃ© (compromis biais-variance)\n",
    "ax = axes[1]\n",
    "degrees = list(poly_est.cv_scores.keys())\n",
    "mse_vals = [poly_est.cv_scores[d] for d in degrees]\n",
    "ax.semilogy(degrees, mse_vals, 'o-', color=COLORS['poly'], lw=2, ms=5)\n",
    "ax.axvline(poly_est.best_degree, color='red', lw=1.5, linestyle='--',\n",
    "           label=f'Optimal d={poly_est.best_degree}')\n",
    "ax.set_xlabel('DegrÃ© $d$', fontsize=11)\n",
    "ax.set_ylabel('MSE CV (log)', fontsize=11)\n",
    "ax.set_title('SÃ©lection de degrÃ© par validation croisÃ©e', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (c) Convergence vs n\n",
    "ax = axes[2]\n",
    "ns_poly = [50, 100, 200, 400, 700, 1000, 2000, 5000]\n",
    "err_poly = []\n",
    "for n in ns_poly:\n",
    "    Xp, yp, _ = generate_dataset(n, SIGMA, seed=42)\n",
    "    est = PolynomialPiEstimator(max_degree=12, alpha=1e-12)\n",
    "    est.fit(Xp, yp)\n",
    "    err_poly.append(abs(est.pi_hat - PI))\n",
    "\n",
    "ax.loglog(ns_poly, err_poly, 's-', color=COLORS['poly'], lw=2, ms=6, label='Poly (d optimal)')\n",
    "ref_x = np.array([ns_poly[0], ns_poly[-1]])\n",
    "ax.loglog(ref_x, 5e-2 * (ref_x/ns_poly[0])**(-1.5), 'k--', lw=1.5, label='$O(n^{-1.5})$')\n",
    "ax.set_xlabel('$n$ (taille du dataset)', fontsize=11)\n",
    "ax.set_ylabel('$|\\hat{\\\\pi} - \\pi|$', fontsize=11)\n",
    "ax.set_title('Convergence de la rÃ©gression polynomiale', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_polynomial.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. MÃ©thode 3 : RÃ©seau de Neurones MLP (from scratch)\n",
    "\n",
    "**ThÃ©orÃ¨me d'approximation universelle (Cybenko 1989) :**  \n",
    "Pour toute $f \\in C([0,1])$ et tout $\\varepsilon > 0$, $\\exists$ MLP Ã  1 couche cachÃ©e tel que $\\|f_{\\theta} - f\\|_\\infty < \\varepsilon$.\n",
    "\n",
    "**Architecture :** $x \\in \\mathbb{R} \\to [64] \\to [64] \\to [32] \\to \\mathbb{R}$, activation GELU\n",
    "\n",
    "**Optimisation :** Adam avec $\\eta=5\\times10^{-4}$, $\\beta_1=0.9$, $\\beta_2=0.999$\n",
    "\n",
    "**Extraction de Ï€ :** $\\hat{\\pi}_{\\text{MLP}} = 4 \\times \\frac{1}{M}\\sum_{j=1}^M f_\\theta(x_j^*)$ (rÃ¨gle des trapÃ¨zes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ImplÃ©mentation MLP NumPy pur (sans torch/tensorflow) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit : x * Î¦(x)\"\"\"\n",
    "    return x * 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def gelu_grad(x):\n",
    "    \"\"\"DÃ©rivÃ©e de GELU.\"\"\"\n",
    "    tanh_val = np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3))\n",
    "    dtanh = 1 - tanh_val**2\n",
    "    d_inner = np.sqrt(2/np.pi) * (1 + 3 * 0.044715 * x**2)\n",
    "    return 0.5 * (1 + tanh_val) + x * 0.5 * dtanh * d_inner\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    RÃ©seau de neurones MLP implÃ©mentÃ© en NumPy pur.\n",
    "    \n",
    "    Architecture : Input(1) -> Hidden layers -> Output(1)\n",
    "    Activation : GELU (couches cachÃ©es) + identitÃ© (sortie)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: list, seed: int = 0):\n",
    "        \"\"\"\n",
    "        ParamÃ¨tres\n",
    "        ----------\n",
    "        layer_sizes : liste des dimensions, ex. [1, 64, 64, 32, 1]\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1  # nombre de couches\n",
    "        \n",
    "        # Initialisation He (Kaiming) : var(W) = 2/n_in\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        for i in range(self.L):\n",
    "            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n",
    "            std = np.sqrt(2.0 / n_in)\n",
    "            self.W.append(rng.normal(0, std, (n_in, n_out)))\n",
    "            self.b.append(np.zeros(n_out))\n",
    "        \n",
    "        # Ã‰tat Adam\n",
    "        self.mW = [np.zeros_like(w) for w in self.W]\n",
    "        self.vW = [np.zeros_like(w) for w in self.W]\n",
    "        self.mb = [np.zeros_like(b) for b in self.b]\n",
    "        self.vb = [np.zeros_like(b) for b in self.b]\n",
    "        self.t  = 0\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> tuple:\n",
    "        \"\"\"\n",
    "        Propagation avant.\n",
    "        Retourne : (output, liste des pre-activations, liste des activations)\n",
    "        \"\"\"\n",
    "        zs, as_ = [], []\n",
    "        a = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "        for i in range(self.L - 1):\n",
    "            z = a @ self.W[i] + self.b[i]\n",
    "            zs.append(z)\n",
    "            a = gelu(z)\n",
    "            as_.append(a)\n",
    "        # Couche de sortie (linÃ©aire)\n",
    "        z = a @ self.W[-1] + self.b[-1]\n",
    "        zs.append(z)\n",
    "        as_.append(z)  # identitÃ©\n",
    "        return z, zs, as_\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        out, _, _ = self.forward(X)\n",
    "        return out.flatten()\n",
    "    \n",
    "    def backward(self, X: np.ndarray, y: np.ndarray) -> tuple:\n",
    "        \"\"\"RÃ©tropropagation du gradient (perte MSE).\"\"\"\n",
    "        n = len(y)\n",
    "        X_in = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "        y_hat, zs, as_ = self.forward(X_in)\n",
    "        \n",
    "        loss = np.mean((y_hat.flatten() - y)**2)\n",
    "        \n",
    "        # Gradient couche de sortie\n",
    "        delta = 2 * (y_hat.flatten() - y).reshape(-1, 1) / n  # (n, 1)\n",
    "        \n",
    "        dW = [None] * self.L\n",
    "        db = [None] * self.L\n",
    "        \n",
    "        # Couche de sortie\n",
    "        a_prev = as_[-2] if self.L > 1 else X_in\n",
    "        dW[-1] = a_prev.T @ delta\n",
    "        db[-1] = delta.sum(axis=0)\n",
    "        \n",
    "        # Couches cachÃ©es (de L-2 Ã  0)\n",
    "        for i in range(self.L - 2, -1, -1):\n",
    "            delta = (delta @ self.W[i+1].T) * gelu_grad(zs[i])\n",
    "            a_prev = as_[i-1] if i > 0 else X_in\n",
    "            dW[i] = a_prev.T @ delta\n",
    "            db[i] = delta.sum(axis=0)\n",
    "        \n",
    "        return loss, dW, db\n",
    "    \n",
    "    def adam_step(self, dW: list, db: list, lr: float = 5e-4,\n",
    "                  beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "        \"\"\"Mise Ã  jour Adam (Kingma & Ba, 2014).\"\"\"\n",
    "        self.t += 1\n",
    "        for i in range(self.L):\n",
    "            # Moments poids\n",
    "            self.mW[i] = beta1 * self.mW[i] + (1 - beta1) * dW[i]\n",
    "            self.vW[i] = beta2 * self.vW[i] + (1 - beta2) * dW[i]**2\n",
    "            mW_hat = self.mW[i] / (1 - beta1**self.t)\n",
    "            vW_hat = self.vW[i] / (1 - beta2**self.t)\n",
    "            self.W[i] -= lr * mW_hat / (np.sqrt(vW_hat) + eps)\n",
    "            # Moments biais\n",
    "            self.mb[i] = beta1 * self.mb[i] + (1 - beta1) * db[i]\n",
    "            self.vb[i] = beta2 * self.vb[i] + (1 - beta2) * db[i]**2\n",
    "            mb_hat = self.mb[i] / (1 - beta1**self.t)\n",
    "            vb_hat = self.vb[i] / (1 - beta2**self.t)\n",
    "            self.b[i] -= lr * mb_hat / (np.sqrt(vb_hat) + eps)\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray,\n",
    "              n_epochs: int = 3000, batch_size: int = 64,\n",
    "              lr: float = 5e-4, verbose_every: int = 500) -> list:\n",
    "        \"\"\"Boucle d'entraÃ®nement SGD-Adam avec mini-batches.\"\"\"\n",
    "        n = len(X)\n",
    "        rng = np.random.default_rng(99)\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle\n",
    "            idx = rng.permutation(n)\n",
    "            X_shuf, y_shuf = X[idx], y[idx]\n",
    "            epoch_loss = 0.0\n",
    "            n_batches  = 0\n",
    "            for start in range(0, n, batch_size):\n",
    "                Xb = X_shuf[start:start+batch_size]\n",
    "                yb = y_shuf[start:start+batch_size]\n",
    "                loss, dW, db = self.backward(Xb, yb)\n",
    "                self.adam_step(dW, db, lr=lr)\n",
    "                epoch_loss += loss\n",
    "                n_batches  += 1\n",
    "            losses.append(epoch_loss / n_batches)\n",
    "            if (epoch + 1) % verbose_every == 0:\n",
    "                print(f\"  Ã‰poque {epoch+1:4d}/{n_epochs} â€” Loss MSE : {losses[-1]:.6f}\")\n",
    "        return losses\n",
    "    \n",
    "    def estimate_pi(self, M: int = 10000) -> float:\n",
    "        \"\"\"Estime Ï€ par rÃ¨gle des trapÃ¨zes sur la prÃ©diction du rÃ©seau.\"\"\"\n",
    "        x_quad = np.linspace(0, 1, M)\n",
    "        f_pred  = self.predict(x_quad)\n",
    "        return 4.0 * np.trapz(f_pred, x_quad)\n",
    "\n",
    "\n",
    "# â”€â”€ EntraÃ®nement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"EntraÃ®nement du MLP...\")\n",
    "t0 = time.time()\n",
    "mlp = MLP(layer_sizes=[1, 64, 64, 32, 1], seed=7)\n",
    "losses = mlp.train(X_train, y_train, n_epochs=3000, batch_size=64, lr=5e-4, verbose_every=1000)\n",
    "t_mlp = time.time() - t0\n",
    "\n",
    "pi_mlp = mlp.estimate_pi(M=50000)\n",
    "print(f\"\\nRÃ©sultats MLP :\")\n",
    "print(f\"  Ï€Ì‚_MLP = {pi_mlp:.10f}\")\n",
    "print(f\"  Erreur absolue : {abs(pi_mlp - PI):.2e}\")\n",
    "print(f\"  Temps d'entraÃ®nement : {t_mlp:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualisation MLP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# (a) Courbe d'entraÃ®nement\n",
    "ax = axes[0]\n",
    "ax.semilogy(losses, color=COLORS['mlp'], lw=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Ã‰poque', fontsize=11)\n",
    "ax.set_ylabel('MSE Loss (log)', fontsize=11)\n",
    "ax.set_title('Courbe de perte (Adam)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.65, 0.7, f'Loss finale\\n{losses[-1]:.2e}', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=9)\n",
    "\n",
    "# (b) Ajustement MLP\n",
    "ax = axes[1]\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "ax.scatter(X_train, y_train, s=8, alpha=0.4, color=COLORS['mlp'], zorder=2, label='DonnÃ©es')\n",
    "ax.plot(X_test, y_exact, color=COLORS['true'], lw=2.5, label='$f(x)$ exacte')\n",
    "ax.plot(X_test, y_pred_mlp, '--', color=COLORS['mlp'], lw=2,\n",
    "        label=f'MLP (3 couches)')\n",
    "residuals = y_pred_mlp - y_exact\n",
    "ax.fill_between(X_test, y_exact + residuals, y_exact, alpha=0.3, color=COLORS['mlp'], label='RÃ©sidu')\n",
    "ax.set_xlabel('$x$', fontsize=11); ax.set_ylabel('$y$', fontsize=11)\n",
    "ax.set_title(f'Ajustement MLP\\n$\\hat{{\\\\pi}}$={pi_mlp:.6f}', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# (c) Distribution des rÃ©sidus\n",
    "ax = axes[2]\n",
    "ax.hist(residuals, bins=40, density=True, color=COLORS['mlp'], alpha=0.7,\n",
    "        edgecolor='white', label='RÃ©sidus MLP')\n",
    "x_res = np.linspace(residuals.min(), residuals.max(), 200)\n",
    "from scipy.stats import norm as norm_dist\n",
    "ax.plot(x_res, norm_dist.pdf(x_res, residuals.mean(), residuals.std()),\n",
    "        'r-', lw=2, label=r'$\\mathcal{N}(\\mu,\\sigma^2)$ ajustÃ©e')\n",
    "ax.axvline(0, color='k', lw=1.5, linestyle='--')\n",
    "ax.set_xlabel('RÃ©sidu $f_{\\\\theta}(x) - f(x)$', fontsize=11)\n",
    "ax.set_ylabel('DensitÃ©', fontsize=11)\n",
    "ax.set_title(f'Distribution des rÃ©sidus\\n($\\mu$={residuals.mean():.2e}, $\\sigma$={residuals.std():.2e})',\n",
    "             fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_mlp.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MÃ©thode 4 : Processus Gaussien (InfÃ©rence BayÃ©sienne)\n",
    "\n",
    "**ModÃ¨le :** $f \\sim \\mathcal{GP}(0, k_{\\text{RBF}})$ avec $k_{\\text{RBF}}(x,x') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$\n",
    "\n",
    "**PostÃ©rieure exacte :**\n",
    "$$\\mu_*(x_*) = \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 I)^{-1} \\mathbf{y}, \\qquad \\sigma_*^2(x_*) = k(x_*,x_*) - \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 I)^{-1} \\mathbf{k}_*$$\n",
    "\n",
    "**HyperparamÃ¨tres :** OptimisÃ©s par maximisation de la log-vraisemblance marginale (MLE-II)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessRegressor:\n",
    "    \"\"\"\n",
    "    RÃ©gression par Processus Gaussien â€” noyau RBF (MatÃ©rn âˆ).\n",
    "    ImplÃ©mentation NumPy avec optimisation MLE-II des hyperparamÃ¨tres.\n",
    "    \n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    sigma_f  : amplitude du noyau (signal std)\n",
    "    length_scale : Ã©chelle de longueur â„“\n",
    "    sigma_n  : bruit d'observation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma_f: float = 1.0, length_scale: float = 0.3,\n",
    "                 sigma_n: float = 0.01):\n",
    "        self.log_params = np.log([sigma_f, length_scale, sigma_n])\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.L_chol = None   # Facteur de Cholesky de (K + Ïƒ_nÂ²I)\n",
    "        self.alpha  = None   # (K + Ïƒ_nÂ²I)â»Â¹ y\n",
    "    \n",
    "    @property\n",
    "    def sigma_f(self): return np.exp(self.log_params[0])\n",
    "    @property\n",
    "    def length_scale(self): return np.exp(self.log_params[1])\n",
    "    @property\n",
    "    def sigma_n(self): return np.exp(self.log_params[2])\n",
    "    \n",
    "    def rbf_kernel(self, X1: np.ndarray, X2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Noyau RBF : k(x,x') = Ïƒ_fÂ² exp(-||x-x'||Â²/(2â„“Â²))\"\"\"\n",
    "        X1, X2 = X1.reshape(-1,1), X2.reshape(-1,1)\n",
    "        dists = ((X1 - X2.T)**2)\n",
    "        return self.sigma_f**2 * np.exp(-dists / (2 * self.length_scale**2))\n",
    "    \n",
    "    def neg_log_marginal_likelihood(self, log_params: np.ndarray) -> float:\n",
    "        \"\"\"Log-vraisemblance marginale nÃ©gative (pour minimisation).\"\"\"\n",
    "        self.log_params = log_params\n",
    "        n = len(self.y_train)\n",
    "        K = self.rbf_kernel(self.X_train, self.X_train)\n",
    "        C = K + (self.sigma_n**2 + 1e-6) * np.eye(n)\n",
    "        try:\n",
    "            L = np.linalg.cholesky(C)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return 1e10\n",
    "        alpha = np.linalg.solve(L.T, np.linalg.solve(L, self.y_train))\n",
    "        # log p(y|X,Î¸) = -Â½ y^T Î± - Î£ log(L_ii) - n/2 log(2Ï€)\n",
    "        nlml = 0.5 * self.y_train @ alpha + np.sum(np.log(np.diag(L))) + 0.5 * n * np.log(2*np.pi)\n",
    "        return nlml\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray,\n",
    "            optimize: bool = True, n_restarts: int = 5) -> 'GaussianProcessRegressor':\n",
    "        \"\"\"EntraÃ®ne le GP avec optimisation MLE-II des hyperparamÃ¨tres.\"\"\"\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        \n",
    "        if optimize:\n",
    "            best_nlml = np.inf\n",
    "            best_params = self.log_params.copy()\n",
    "            rng = np.random.default_rng(42)\n",
    "            for _ in range(n_restarts):\n",
    "                x0 = rng.uniform(np.log(0.01), np.log(2.0), 3)\n",
    "                result = minimize(self.neg_log_marginal_likelihood, x0,\n",
    "                                  method='L-BFGS-B',\n",
    "                                  bounds=[(-5, 2), (-5, 1), (-6, 0)])\n",
    "                if result.fun < best_nlml:\n",
    "                    best_nlml = result.fun\n",
    "                    best_params = result.x.copy()\n",
    "            self.log_params = best_params\n",
    "        \n",
    "        # PrÃ©calcul de la factorisation de Cholesky\n",
    "        n = len(y)\n",
    "        K = self.rbf_kernel(X, X)\n",
    "        C = K + (self.sigma_n**2 + 1e-6) * np.eye(n)\n",
    "        self.L_chol, _ = cho_factor(C, lower=True)\n",
    "        self.alpha = cho_solve((self.L_chol, True), y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_star: np.ndarray, return_std: bool = True) -> tuple:\n",
    "        \"\"\"\n",
    "        Calcule la moyenne et variance a posteriori.\n",
    "        Î¼*(x*) = k*^T Î±\n",
    "        Ïƒ*Â²(x*) = k(x*,x*) - k*^T (K+Ïƒ_nÂ²I)^{-1} k*\n",
    "        \"\"\"\n",
    "        k_star = self.rbf_kernel(X_star, self.X_train)  # (M, n)\n",
    "        mu = k_star @ self.alpha\n",
    "        \n",
    "        if return_std:\n",
    "            v = cho_solve((self.L_chol, True), k_star.T)  # (n, M)\n",
    "            var = self.sigma_f**2 - np.sum(k_star.T * v, axis=0)\n",
    "            var = np.maximum(var, 1e-10)\n",
    "            return mu, np.sqrt(var)\n",
    "        return mu, None\n",
    "    \n",
    "    def estimate_pi(self, M: int = 10000) -> tuple:\n",
    "        \"\"\"Estime Ï€ et son incertitude bayÃ©sienne.\"\"\"\n",
    "        x_quad = np.linspace(0, 1, M)\n",
    "        mu, sigma = self.predict(x_quad)\n",
    "        dx = x_quad[1] - x_quad[0]\n",
    "        # Ï€Ì‚ = 4 âˆ« Î¼*(x)dx\n",
    "        pi_hat = 4.0 * np.trapz(mu, x_quad)\n",
    "        # Ïƒ_Ï€ â‰ˆ 4âˆš(âˆ« ÏƒÂ²*(x)dx) (borne conservative)\n",
    "        pi_std = 4.0 * np.sqrt(np.sum(sigma**2) * dx)\n",
    "        return pi_hat, pi_std\n",
    "\n",
    "\n",
    "# â”€â”€ EntraÃ®nement GP (sous-ensemble de n=200 points pour la rapiditÃ©) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# La complexitÃ© GP est O(nÂ³), on utilise un sous-ensemble stratifiÃ©\n",
    "N_GP = 200\n",
    "idx_gp = np.linspace(0, N_TRAIN-1, N_GP, dtype=int)\n",
    "X_gp, y_gp = X_train[idx_gp], y_train[idx_gp]\n",
    "\n",
    "print(\"EntraÃ®nement du Processus Gaussien (MLE-II pour hyperparamÃ¨tres)...\")\n",
    "t0 = time.time()\n",
    "gp = GaussianProcessRegressor(sigma_f=1.0, length_scale=0.2, sigma_n=SIGMA)\n",
    "gp.fit(X_gp, y_gp, optimize=True, n_restarts=8)\n",
    "t_gp = time.time() - t0\n",
    "\n",
    "pi_gp, pi_gp_std = gp.estimate_pi(M=10000)\n",
    "print(f\"\\nHyperparamÃ¨tres optimisÃ©s :\")\n",
    "print(f\"  Ïƒ_f = {gp.sigma_f:.4f}  |  â„“ = {gp.length_scale:.4f}  |  Ïƒ_n = {gp.sigma_n:.5f}\")\n",
    "print(f\"\\nRÃ©sultats GP :\")\n",
    "print(f\"  Ï€Ì‚_GP = {pi_gp:.10f}\")\n",
    "print(f\"  Erreur absolue : {abs(pi_gp - PI):.2e}\")\n",
    "print(f\"  IC bayÃ©sien (Â±2Ïƒ) : [{pi_gp - 2*pi_gp_std:.8f}, {pi_gp + 2*pi_gp_std:.8f}]\")\n",
    "print(f\"  Temps : {t_gp:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualisation GP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mu_gp, std_gp = gp.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (a) PostÃ©rieure GP avec bande d'incertitude\n",
    "ax = axes[0]\n",
    "ax.scatter(X_gp, y_gp, s=10, zorder=5, color=COLORS['gp'],\n",
    "           alpha=0.6, label=f'DonnÃ©es ({N_GP} pts)')\n",
    "ax.plot(X_test, y_exact, color=COLORS['true'], lw=2.5, label='$f(x)$ exacte')\n",
    "ax.plot(X_test, mu_gp, color=COLORS['gp'], lw=2.5, label='Moyenne postÃ©rieure $\\mu_*(x)$')\n",
    "ax.fill_between(X_test, mu_gp - 2*std_gp, mu_gp + 2*std_gp,\n",
    "                alpha=0.3, color=COLORS['gp'], label='IC 95% ($\\pm 2\\sigma_*$)')\n",
    "ax.fill_between(X_test, mu_gp - std_gp, mu_gp + std_gp,\n",
    "                alpha=0.4, color=COLORS['gp'])\n",
    "ax.set_xlabel('$x$', fontsize=12); ax.set_ylabel('$y$', fontsize=12)\n",
    "ax.set_title(f'GP PostÃ©rieure\\n$\\hat{{\\\\pi}}$ = {pi_gp:.6f} Â± {pi_gp_std:.2e}', fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='lower left')\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "\n",
    "# (b) Incertitude postÃ©rieure Ïƒ*(x)\n",
    "ax = axes[1]\n",
    "ax.plot(X_test, std_gp, color=COLORS['gp'], lw=2)\n",
    "ax.fill_between(X_test, 0, std_gp, alpha=0.3, color=COLORS['gp'])\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.set_ylabel('$\\sigma_*(x)$ (Ã©cart-type postÃ©rieur)', fontsize=11)\n",
    "ax.set_title('Incertitude Ã©pistÃ©mique du GP\\n(faible lÃ  oÃ¹ des donnÃ©es existent)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "# Ajouter les positions des points d'entraÃ®nement\n",
    "ax.scatter(X_gp, np.zeros_like(X_gp) - 2e-5, s=8, color=COLORS['gp'],\n",
    "           alpha=0.5, zorder=5, label='Points d\\'entraÃ®nement')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_gp.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparaison Globale des MÃ©thodes\n",
    "\n",
    "Cette section effectue une analyse comparative rigoureuse selon trois critÃ¨res :\n",
    "1. **PrÃ©cision** : erreur absolue $|\\hat{\\pi} - \\pi|$\n",
    "2. **Convergence** : taux empirique en fonction de $n$\n",
    "3. **CoÃ»t computationnel** : temps CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RÃ©sumÃ© des rÃ©sultats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = {\n",
    "    'Monte-Carlo\\n(n=100k)': {\n",
    "        'pi': pi_mc_final, 'error': abs(pi_mc_final - PI),\n",
    "        'time': 0.015, 'color': COLORS['mc']\n",
    "    },\n",
    "    f'Poly. (d={poly_est.best_degree})': {\n",
    "        'pi': poly_est.pi_hat, 'error': abs(poly_est.pi_hat - PI),\n",
    "        'time': t_poly, 'color': COLORS['poly']\n",
    "    },\n",
    "    'MLP\\n(3 couches, n=300)': {\n",
    "        'pi': pi_mlp, 'error': abs(pi_mlp - PI),\n",
    "        'time': t_mlp, 'color': COLORS['mlp']\n",
    "    },\n",
    "    'Processus\\nGaussien': {\n",
    "        'pi': pi_gp, 'error': abs(pi_gp - PI),\n",
    "        'time': t_gp, 'color': COLORS['gp']\n",
    "    },\n",
    "}\n",
    "\n",
    "# â”€â”€ Tableau rÃ©capitulatif â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â•\" * 75)\n",
    "print(f\"{'MÃ©thode':<30} {'Ï€Ì‚':>14} {'Erreur abs.':>14} {'Temps (s)':>10}\")\n",
    "print(\"â•\" * 75)\n",
    "for name, r in results.items():\n",
    "    name_clean = name.replace('\\n', ' ')\n",
    "    print(f\"{name_clean:<30} {r['pi']:>14.10f} {r['error']:>14.2e} {r['time']:>10.3f}\")\n",
    "print(\"â”€\" * 75)\n",
    "print(f\"{'Ï€ (valeur exacte)':<30} {PI:>14.10f}\")\n",
    "print(\"â•\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Convergence comparative â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ns_comp = [50, 100, 200, 400, 1000, 2000, 5000]\n",
    "N_SEEDS = 5  # Moyenne sur plusieurs seeds pour la robustesse\n",
    "\n",
    "all_errors = {'mc': [], 'poly': [], 'gp': []}\n",
    "\n",
    "for n in ns_comp:\n",
    "    errs = {'mc': [], 'poly': [], 'gp': []}\n",
    "    for seed in range(N_SEEDS):\n",
    "        # Monte-Carlo\n",
    "        pi_hat_mc, _, _, _ = monte_carlo_pi(n * 10, seed=seed)  # MC a besoin de plus de points\n",
    "        errs['mc'].append(abs(pi_hat_mc - PI))\n",
    "        \n",
    "        # Poly\n",
    "        Xp, yp, _ = generate_dataset(n, SIGMA, seed=seed)\n",
    "        try:\n",
    "            ep = PolynomialPiEstimator(max_degree=10, alpha=1e-12)\n",
    "            ep.fit(Xp, yp)\n",
    "            errs['poly'].append(abs(ep.pi_hat - PI))\n",
    "        except:\n",
    "            errs['poly'].append(np.nan)\n",
    "        \n",
    "        # GP (sous-ensemble)\n",
    "        n_gp_comp = min(n, 150)\n",
    "        Xg, yg, _ = generate_dataset(n_gp_comp, SIGMA, seed=seed)\n",
    "        try:\n",
    "            gp_comp = GaussianProcessRegressor(sigma_f=1.0, length_scale=0.2, sigma_n=SIGMA)\n",
    "            gp_comp.fit(Xg, yg, optimize=False)  # Sans optim pour la rapiditÃ©\n",
    "            pi_g, _ = gp_comp.estimate_pi(M=5000)\n",
    "            errs['gp'].append(abs(pi_g - PI))\n",
    "        except:\n",
    "            errs['gp'].append(np.nan)\n",
    "    \n",
    "    for k in all_errors:\n",
    "        arr = np.array(errs[k])\n",
    "        all_errors[k].append(np.nanmean(arr))\n",
    "\n",
    "all_errors = {k: np.array(v) for k, v in all_errors.items()}\n",
    "\n",
    "# â”€â”€ Figure finale : comparaison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs  = gridspec.GridSpec(2, 3, figure=fig, hspace=0.4, wspace=0.35)\n",
    "\n",
    "# (1) Convergence log-log\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ns_mc = [n*10 for n in ns_comp]\n",
    "ax1.loglog(ns_mc, all_errors['mc'], 'o-', color=COLORS['mc'], lw=2, ms=7,\n",
    "           label='Monte-Carlo')\n",
    "ax1.loglog(ns_comp, all_errors['poly'], 's-', color=COLORS['poly'], lw=2, ms=7,\n",
    "           label='RÃ©gression polynomiale')\n",
    "ax1.loglog(ns_comp, all_errors['gp'], 'd-', color=COLORS['gp'], lw=2, ms=7,\n",
    "           label='Processus Gaussien')\n",
    "\n",
    "# RÃ©fÃ©rences thÃ©oriques\n",
    "ref_x = np.array([50, 50000])\n",
    "ax1.loglog(ref_x, 3.0 * ref_x**(-0.5), '--', color=COLORS['mc'], alpha=0.5, lw=1.5,\n",
    "           label=r'RÃ©f. $O(n^{-1/2})$')\n",
    "ax1.loglog(ref_x[0:1]*np.array([1, 100]),\n",
    "           [all_errors['poly'][0], all_errors['poly'][0]*0.001],\n",
    "           '--', color=COLORS['poly'], alpha=0.5, lw=1.5, label=r'RÃ©f. $O(n^{-1.5})$')\n",
    "ax1.set_xlabel('$n$ (taille du dataset / nbre tirages MC)', fontsize=12)\n",
    "ax1.set_ylabel('Erreur absolue $|\\hat{\\\\pi}-\\pi|$', fontsize=12)\n",
    "ax1.set_title('Comparaison des taux de convergence (log-log)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right', ncol=2)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.axhline(1e-4, color='gray', lw=0.8, alpha=0.5, linestyle=':')\n",
    "ax1.text(50, 1.2e-4, 'Seuil $10^{-4}$', fontsize=8, color='gray')\n",
    "\n",
    "# (2) Barres d'erreur finale\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "labels = [k.replace('\\n', ' ') for k in results.keys()]\n",
    "errors_final = [r['error'] for r in results.values()]\n",
    "colors_bar   = [r['color'] for r in results.values()]\n",
    "bars = ax2.bar(range(len(labels)), errors_final, color=colors_bar, alpha=0.8, edgecolor='white')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xticks(range(len(labels)))\n",
    "ax2.set_xticklabels(labels, fontsize=8)\n",
    "ax2.set_ylabel('$|\\hat{\\\\pi} - \\pi|$ (log)', fontsize=10)\n",
    "ax2.set_title('Erreur absolue finale', fontweight='bold')\n",
    "for bar, err in zip(bars, errors_final):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height()*1.2,\n",
    "             f'{err:.1e}', ha='center', va='bottom', fontsize=7.5)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# (3) FrontiÃ¨re Pareto prÃ©cision / temps\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "for name, r in results.items():\n",
    "    n_clean = name.replace('\\n', ' ')\n",
    "    ax3.scatter(r['time'], r['error'], s=120, color=r['color'],\n",
    "                zorder=5, edgecolors='white', linewidths=1.5)\n",
    "    ax3.annotate(n_clean, (r['time'], r['error']),\n",
    "                 textcoords='offset points', xytext=(5, 5), fontsize=7.5)\n",
    "ax3.set_xscale('log'); ax3.set_yscale('log')\n",
    "ax3.set_xlabel('Temps CPU (s, log)', fontsize=10)\n",
    "ax3.set_ylabel('$|\\hat{\\\\pi} - \\pi|$ (log)', fontsize=10)\n",
    "ax3.set_title('FrontiÃ¨re de Pareto\\nPrÃ©cision vs CoÃ»t', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# (4) Distributions des estimateurs\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "estimates = {\n",
    "    'MC': [monte_carlo_pi(1000, seed=s)[0] for s in range(200)],\n",
    "    'Poly': [],\n",
    "    'GP': [],\n",
    "}\n",
    "for seed in range(50):\n",
    "    Xp, yp, _ = generate_dataset(300, SIGMA, seed=seed)\n",
    "    try:\n",
    "        ep = PolynomialPiEstimator(max_degree=10, alpha=1e-12)\n",
    "        ep.fit(Xp, yp)\n",
    "        estimates['Poly'].append(ep.pi_hat)\n",
    "    except: pass\n",
    "    try:\n",
    "        gp_s = GaussianProcessRegressor(sigma_f=1.0, length_scale=0.2, sigma_n=SIGMA)\n",
    "        gp_s.fit(Xp[:100], yp[:100], optimize=False)\n",
    "        pi_g_s, _ = gp_s.estimate_pi(M=2000)\n",
    "        estimates['GP'].append(pi_g_s)\n",
    "    except: pass\n",
    "\n",
    "for key, col, lw in zip(['MC', 'Poly', 'GP'],\n",
    "                         [COLORS['mc'], COLORS['poly'], COLORS['gp']],\n",
    "                         [1.5, 2, 2]):\n",
    "    vals = np.array(estimates[key])\n",
    "    ax4.hist(vals, bins=30, density=True, alpha=0.5, color=col,\n",
    "             edgecolor='white', label=key)\n",
    "ax4.axvline(PI, color='k', lw=2, linestyle='--', label=f'Ï€ = {PI:.4f}')\n",
    "ax4.set_xlabel('$\\hat{\\\\pi}$', fontsize=10)\n",
    "ax4.set_ylabel('DensitÃ©', fontsize=10)\n",
    "ax4.set_title('Distribution des estimateurs\\n(variance et biais)', fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Analyse Comparative : Approximation de $\\pi$ par Machine Learning',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.savefig('fig_comparaison.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure de comparaison sauvegardÃ©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ã‰tude ThÃ©orique : ComplexitÃ© de Rademacher\n",
    "\n",
    "Cette section calcule et visualise la **complexitÃ© de Rademacher empirique** $\\hat{\\mathfrak{R}}_n(\\mathcal{H})$ pour la classe polynomiale, qui borne l'erreur de gÃ©nÃ©ralisation (Bartlett & Mendelson, 2002) :\n",
    "$$\\mathcal{R}(f) \\leq \\hat{\\mathcal{R}}_n(f) + 2\\hat{\\mathfrak{R}}_n(\\mathcal{H}) + \\sqrt{\\frac{\\log(1/\\delta)}{2n}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_complexity(X: np.ndarray, degree: int,\n",
    "                           n_mc: int = 500, seed: int = 0) -> float:\n",
    "    \"\"\"\n",
    "    Estime la complexitÃ© de Rademacher empirique pour la classe polynomiale.\n",
    "    \n",
    "    ğ”‘Ì‚_n(H) = E_Ïƒ[sup_{||Î¸||â‰¤1} (1/n) Î£_i Ïƒ_i p_Î¸(x_i)]\n",
    "    Pour les polynÃ´mes normalisÃ©s, cela se rÃ©duit Ã  (1/n) ||Î¦||_F / âˆš(d+1)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n   = len(X)\n",
    "    # Matrice de features de Vandermonde normalisÃ©e\n",
    "    Phi = np.column_stack([X**k for k in range(degree+1)])  # (n, d+1)\n",
    "    \n",
    "    # Estimation Monte-Carlo : E[sup_{||Î¸||_2<=1} (1/n) Ïƒ^T Î¦ Î¸]\n",
    "    # = E[(1/n) ||Î¦^T Ïƒ||_2] (le sup est atteint par Î¸=Î¦^TÏƒ/||Î¦^TÏƒ||)\n",
    "    sups = []\n",
    "    for _ in range(n_mc):\n",
    "        sigma = rng.choice([-1., 1.], size=n)\n",
    "        s = np.linalg.norm(Phi.T @ sigma) / n\n",
    "        sups.append(s)\n",
    "    return np.mean(sups)\n",
    "\n",
    "\n",
    "degrees = range(1, 14)\n",
    "ns_rad  = [50, 200, 500]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (a) Rademacher vs degrÃ© pour diffÃ©rents n\n",
    "ax = axes[0]\n",
    "for n, ls in zip(ns_rad, ['-', '--', ':']):\n",
    "    X_rad, _, _ = generate_dataset(n, 0, seed=42)\n",
    "    rad = [rademacher_complexity(X_rad, d, n_mc=200) for d in degrees]\n",
    "    ax.plot(list(degrees), rad, ls, lw=2, label=f'$n={n}$')\n",
    "\n",
    "ax.set_xlabel('DegrÃ© $d$', fontsize=11)\n",
    "ax.set_ylabel(r'$\\hat{\\mathfrak{R}}_n(\\mathcal{P}_d)$', fontsize=11)\n",
    "ax.set_title('ComplexitÃ© de Rademacher\\nvs degrÃ© polynomial', fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Borne de gÃ©nÃ©ralisation PAC\n",
    "ax = axes[1]\n",
    "delta = 0.05\n",
    "n_vals = [50, 100, 200, 500, 1000]\n",
    "d_fixed = 7\n",
    "pac_bounds = []\n",
    "for n in n_vals:\n",
    "    X_b, y_b, _ = generate_dataset(n, SIGMA, seed=42)\n",
    "    pipe = Pipeline([('poly', PolynomialFeatures(degree=d_fixed)),\n",
    "                     ('ridge', Ridge(alpha=1e-12))])\n",
    "    pipe.fit(X_b.reshape(-1,1), y_b)\n",
    "    y_pred_b = pipe.predict(X_b.reshape(-1,1))\n",
    "    emp_risk = np.mean((y_pred_b - y_b)**2)\n",
    "    \n",
    "    rad = rademacher_complexity(X_b, d_fixed, n_mc=100)\n",
    "    conf_term = np.sqrt(np.log(1/delta) / (2*n))\n",
    "    pac_bound = emp_risk + 2*rad + conf_term\n",
    "    pac_bounds.append({'n': n, 'emp': emp_risk, 'rad': rad, 'conf': conf_term, 'bound': pac_bound})\n",
    "\n",
    "n_plot = [p['n'] for p in pac_bounds]\n",
    "ax.loglog(n_plot, [p['bound'] for p in pac_bounds], 'o-', color=COLORS['poly'],\n",
    "          lw=2, ms=8, label='Borne PAC totale')\n",
    "ax.loglog(n_plot, [p['emp'] for p in pac_bounds], 's--', color=COLORS['mc'],\n",
    "          lw=2, ms=6, label='Risque empirique $\\hat{R}_n$')\n",
    "ax.loglog(n_plot, [2*p['rad'] for p in pac_bounds], '^--', color=COLORS['gp'],\n",
    "          lw=2, ms=6, label=r'$2\\hat{\\mathfrak{R}}_n$')\n",
    "ax.loglog(n_plot, [p['conf'] for p in pac_bounds], 'v--', color=COLORS['mlp'],\n",
    "          lw=2, ms=6, label=r'Terme de conf. $\\sqrt{\\log(1/\\delta)/2n}$')\n",
    "\n",
    "ax.set_xlabel('$n$', fontsize=11)\n",
    "ax.set_ylabel('Valeur (log)', fontsize=11)\n",
    "ax.set_title(f'DÃ©composition de la borne PAC ($d={d_fixed}$, $\\delta={delta}$)', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_rademacher.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Analyse thÃ©orique (Rademacher) complÃ©tÃ©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion et SynthÃ¨se\n",
    "\n",
    "| MÃ©thode | HypothÃ¨ses | Taux (thÃ©orique) | Forces | Limites |\n",
    "|---|---|---|---|---|\n",
    "| Monte-Carlo | Aucune (universelle) | $O(n^{-1/2})$ | Simple, parallelisable | Lente convergence |\n",
    "| Poly. OLS | $f$ analytique | $O(\\rho^{-d})$ en degrÃ© | IntÃ©gration exacte | Conditionnement de Vandermonde |\n",
    "| MLP (Adam) | $f \\in$ classe de Barron | $O(m^{-1})$ en neurones | Flexible, haute prÃ©cision | Non-convexe, opaque |\n",
    "| GP (RBF) | $f \\in$ RKHS de $k$ | Expo. en rÃ©gularitÃ© | Incertitude quantifiÃ©e | Cubique en $n$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RÃ©sumÃ© final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘       APPROXIMATION DE Ï€ PAR MACHINE LEARNING â€” BILAN FINAL          â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(f\"â•‘  Valeur de rÃ©fÃ©rence : Ï€ = {PI:<45.15f}â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "\n",
    "methodes = [\n",
    "    ('Monte-Carlo (n=100k)',     pi_mc_final),\n",
    "    (f'Poly. (d={poly_est.best_degree}, n=300)',  poly_est.pi_hat),\n",
    "    ('MLP (n=300, 3000 ep.)',    pi_mlp),\n",
    "    ('GP (n=200)',               pi_gp),\n",
    "]\n",
    "\n",
    "for name, pi_est in methodes:\n",
    "    err = abs(pi_est - PI)\n",
    "    n_correct = max(0, -int(np.floor(np.log10(err))) - 1) if err > 0 else 15\n",
    "    print(f\"â•‘  {name:<28} Ï€Ì‚ = {pi_est:.10f}  err={err:.2e}  â•‘\")\n",
    "\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(\"â•‘  VAINQUEUR (erreur minimale) : Processus Gaussien                    â•‘\")\n",
    "print(\"â•‘  VAINQUEUR (rapport prÃ©cision/temps) : RÃ©gression Polynomiale        â•‘\")\n",
    "print(\"â•‘  RÃ©fÃ©rence : Leibniz-Gregory : 4*(1-1/3+1/5-...) â†’ O(k^{-1})        â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "# SÃ©rie de Leibniz pour comparaison\n",
    "k_vals = np.arange(0, 100_000)\n",
    "leibniz = 4 * np.sum((-1)**k_vals / (2*k_vals + 1))\n",
    "print(f\"\\nSÃ©rie de Leibniz (100k termes) : Ï€Ì‚ = {leibniz:.10f}, err = {abs(leibniz-PI):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
